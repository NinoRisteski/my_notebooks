{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./env/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: sympy in ./env/lib/python3.9/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.9/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.9/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./env/lib/python3.9/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: networkx in ./env/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./env/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.9/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./env/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/Users/fliprise/my_notebooks/env/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pytorch-lightning in ./env/lib/python3.9/site-packages (2.4.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in ./env/lib/python3.9/site-packages (from pytorch-lightning) (1.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.9/site-packages (from pytorch-lightning) (24.1)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in ./env/lib/python3.9/site-packages (from pytorch-lightning) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in ./env/lib/python3.9/site-packages (from pytorch-lightning) (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.4 in ./env/lib/python3.9/site-packages (from pytorch-lightning) (6.0.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.10.0 in ./env/lib/python3.9/site-packages (from pytorch-lightning) (0.11.8)\n",
      "Requirement already satisfied: torch>=2.1.0 in ./env/lib/python3.9/site-packages (from pytorch-lightning) (2.2.2)\n",
      "Requirement already satisfied: fsspec[http]>=2022.5.0 in ./env/lib/python3.9/site-packages (from pytorch-lightning) (2024.10.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in ./env/lib/python3.9/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.10.10)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./env/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./env/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./env/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./env/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./env/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./env/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./env/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.16.0)\n",
      "Requirement already satisfied: setuptools in ./env/lib/python3.9/site-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (56.0.0)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning) (3.1.4)\n",
      "Requirement already satisfied: networkx in ./env/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning) (3.2.1)\n",
      "Requirement already satisfied: sympy in ./env/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning) (1.13.3)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.9/site-packages (from torch>=2.1.0->pytorch-lightning) (3.16.1)\n",
      "Requirement already satisfied: numpy<2.0,>1.20.0 in ./env/lib/python3.9/site-packages (from torchmetrics>=0.7.0->pytorch-lightning) (1.26.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./env/lib/python3.9/site-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.2.0)\n",
      "Requirement already satisfied: idna>=2.0 in ./env/lib/python3.9/site-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.9/site-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./env/lib/python3.9/site-packages (from sympy->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 24.2 is available.\n",
      "You should consider upgrading via the '/Users/fliprise/my_notebooks/env/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "%pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Data Module for PyTorch Lightning\n",
    "\n",
    "class BaseDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=32, split=0.8, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Get the dataset using the get_dataset method (to be implemented in subclasses)\n",
    "        self.ds_x, self.ds_y = self.get_dataset(*args, **kwargs)\n",
    "        \n",
    "        # Create a random permutation of indices to shuffle the dataset\n",
    "        # This ensures that the data is randomly ordered, which is important for training\n",
    "        shuffler = np.random.permutation(self.ds_x.shape[0])\n",
    "        \n",
    "        # Shuffle both input features (ds_x) and labels (ds_y) using the same permutation\n",
    "        # This maintains the correspondence between inputs and labels\n",
    "        self.ds_x = self.ds_x[shuffler]\n",
    "        self.ds_y = self.ds_y[shuffler]\n",
    "        \n",
    "        # Set the batch size for data loading\n",
    "        # This determines how many samples will be processed at once during training\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Calculate the split index for train/validation sets\n",
    "        # split is a float between 0 and 1, representing the proportion of data for training\n",
    "        # This allows for flexible dataset splitting\n",
    "        self.split = int(self.ds_x.shape[0] * split)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # Slice the training data from the beginning up to the split index\n",
    "        ds_X_train, ds_Y_train = self.ds_x[0:self.split], self.ds_y[0:self.split]\n",
    "        # Create and return a DataLoader with zipped training data and labels\n",
    "        # This DataLoader will be used by PyTorch Lightning to fetch batches during training\n",
    "        return torch.utils.data.DataLoader(list(zip(ds_X_train, ds_Y_train)), batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # Slice the validation data from the split index to the end\n",
    "        ds_X_test, ds_Y_test = self.ds_x[self.split:], self.ds_y[self.split:]\n",
    "        # Create and return a DataLoader with zipped validation data and labels\n",
    "        # This DataLoader will be used by PyTorch Lightning to fetch batches during validation\n",
    "        return torch.utils.data.DataLoader(list(zip(ds_X_test, ds_Y_test)), batch_size=self.batch_size)\n",
    "    \n",
    "class ReverseDataModule(BaseDataModule):\n",
    "    def get_dataset(self, cnt=1000, seq_len=6):\n",
    "        # Generate a synthetic dataset for the reverse sequence task\n",
    "        # cnt: Number of samples in the dataset (default 1000)\n",
    "        # seq_len: Length of each sequence (default 6)\n",
    "        \n",
    "        # Create random integer sequences from 0 to 9\n",
    "        ds = np.random.randint(0, 10, size=(cnt, seq_len))\n",
    "        \n",
    "        # Return two arrays:\n",
    "        # 1. The original random sequences (ds)\n",
    "        # 2. The reversed sequences (ds[:, ::-1])\n",
    "        #    - [:, ::-1] reverses each sequence\n",
    "        #    - ravel() flattens the array\n",
    "        #    - reshape() reshapes it back to (cnt, seq_len)\n",
    "        # The reversed sequences serve as the target for the model to learn\n",
    "        return ds, ds[:, ::-1].ravel().reshape(cnt, seq_len)\n",
    "\n",
    "# dataset idea from https://github.com/karpathy/minGPT/blob/master/play_math.ipynb\n",
    "\n",
    "class AdditionDataModule(BaseDataModule):\n",
    "    def get_dataset(self):\n",
    "        # This method generates a dataset for addition problems\n",
    "        \n",
    "        ret = []\n",
    "        # Outer loop: iterates through numbers 0-99 for the first addend\n",
    "        for i in range(100):\n",
    "            # Inner loop: iterates through numbers 0-99 for the second addend\n",
    "            for j in range(100):\n",
    "                # Calculate the sum of i and j\n",
    "                s = i + j\n",
    "                \n",
    "                # Append a list containing:\n",
    "                # - Tens digit of i (i // 10)\n",
    "                # - Ones digit of i (i % 10)\n",
    "                # - Tens digit of j (j // 10)\n",
    "                # - Ones digit of j (j % 10)\n",
    "                # - Hundreds digit of sum (s // 100)\n",
    "                # - Tens digit of sum ((s // 10) % 10)\n",
    "                # - Ones digit of sum (s % 10)\n",
    "                ret.append([i//10, i%10, j//10, j%10, s//100, (s//10)%10, s%10])\n",
    "        \n",
    "        # Convert the list of lists to a numpy array for efficient processing\n",
    "        ds = np.array(ret)\n",
    "        \n",
    "        # Return two arrays:\n",
    "        # 1. Input features: first 6 elements of each row (i and j digits, first two digits of sum)\n",
    "        # 2. Target labels: second element of each row (ones digit of i, which is the target to predict)\n",
    "        return ds[:, 0:6], np.copy(ds[:, 1])\n",
    "    \n",
    "class ParityDataModule(BaseDataModule):\n",
    "    def get_dataset(self, seq_len=50):\n",
    "        \"\"\"\n",
    "        Purpose: Creates a dataset for next-character prediction using the enwik8 dataset\n",
    "        Parameters: seq_len - length of sequences to generate (default 50 characters)\n",
    "        Returns: Input sequences and their corresponding next characters\n",
    "        \"\"\"\n",
    "        # Access or download the enwik8 dataset\n",
    "        global enwik8\n",
    "        if 'enwik8' not in globals():\n",
    "            # Download enwik8 dataset if not already loaded\n",
    "            import requests\n",
    "            enwik8_zipped = requests.get('https://data.deepai.org/enwik8.zip').content\n",
    "            from zipfile import ZipFile\n",
    "            import io\n",
    "            # Extract the dataset from zip file\n",
    "            enwik8 = ZipFile(io.BytesIO(enwik8_zipped)).read('enwik8')\n",
    "        \n",
    "        # Convert bytes to integers and ensure values are in ASCII range\n",
    "        en = np.frombuffer(enwik8, dtype=np.uint8).astype(np.int)\n",
    "        # Remove last seq_len-1 characters to ensure all sequences are complete\n",
    "        en = en[0:-seq_len+1]\n",
    "        # Cap values at 127 (ASCII range)\n",
    "        en[en>127] = 127\n",
    "        \n",
    "        # Return:\n",
    "        # 1. Input sequences: All sequences except last character\n",
    "        # 2. Target sequences: All sequences shifted by one character\n",
    "        # This creates a next-character prediction task\n",
    "        return en[0:-1].reshape(-1, seq_len), en[1:].reshape(-1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention mechanism\n",
    "\n",
    "def attention(queries, keys, values):\n",
    "    # Get the dimension of the query vectors\n",
    "    # This is used for scaling to prevent extremely large values in the attention scores\n",
    "    d = queries.shape[-1]\n",
    "    \n",
    "    # Calculate attention scores using scaled dot product attention:\n",
    "    # 1. Multiply queries with transposed keys to get similarity scores\n",
    "    # 2. Scale by 1/sqrt(d) to prevent extremely large values that could lead to\n",
    "    #    extremely small gradients in softmax\n",
    "    scores = torch.matmul(queries, keys.transpose(-2, -1)) / math.sqrt(d)\n",
    "    \n",
    "    # Apply softmax to get attention weights\n",
    "    # - Converts scores to probabilities (values between 0 and 1 that sum to 1)\n",
    "    # - dim=1 means softmax is applied along the keys dimension\n",
    "    # - These weights determine how much each value contributes to the output\n",
    "    attention_weights = F.softmax(scores, dim=1)\n",
    "    \n",
    "    # Compute weighted sum of values\n",
    "    # - Multiply attention weights with values to get the final attention output\n",
    "    # - Each output position is a weighted combination of all input values\n",
    "    # - Weights determine how much each value contributes to the final output\n",
    "    return torch.matmul(attention_weights, values)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - embed_dim: The dimension of the input embeddings\n",
    "        - num_heads: Number of attention heads to use in parallel\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        # Store the embedding dimension and number of attention heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Ensure the embedding dimension is divisible by the number of heads\n",
    "        # This is necessary for splitting the embeddings across heads\n",
    "        assert embed_dim % num_heads == 0\n",
    "        \n",
    "        # Calculate the dimension for each attention head\n",
    "        # Each head will work with a slice of the embedding\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "\n",
    "        # Linear transformations for Query, Key, and Value matrices\n",
    "        # W_q: Projects input embeddings into query space\n",
    "        # W_k: Projects input embeddings into key space\n",
    "        # W_v: Projects input embeddings into value space\n",
    "        # Each maintains the same dimension as input (embed_dim)\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # Final output projection\n",
    "        # W_o: Combines and projects the concatenated attention head outputs\n",
    "        # back to the original embedding dimension\n",
    "        self.W_o = nn.Linear(embed_dim, embed_dim)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
